{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45042041-4e6d-429e-b024-666e6a984dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import error: No module named 'triton'\n",
      "INFO 08-14 12:22:50 [__init__.py:256] Automatically detected platform cpu.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"  \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\" \n",
    "os.environ[\"PYTORCH_DISABLE_CUDA_GRAPHS\"] = \"1\"  \n",
    "import platform\n",
    "import sqlite3\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "import queue\n",
    "from fastapi.websockets import WebSocketState\n",
    "import torch\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import whisper\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from typing import Optional\n",
    "from generator import Segment, load_csm_1b_local\n",
    "from llm_interface import LLMInterface\n",
    "from rag_system import RAGSystem \n",
    "from vad import AudioStreamProcessor\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "from config import ConfigManager\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ada937-2a55-48f6-bae3-aee80ddb552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaking_start_time = 0.0          # set every time the AI begins a new turn\n",
    "MIN_BARGE_LATENCY   = 0.9   \n",
    "speaker_counters = {\n",
    "    0: 0,  # AI\n",
    "    1: 0   # User\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d0c8e8-0ebf-4754-9d58-4188e67427e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/qn3tpg6s4b1f8bz0f_70p0lr0000gn/T/ipykernel_1536/2227991242.py:18: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "current_generation_id = 1\n",
    "pending_user_inputs = []\n",
    "user_input_lock = threading.Lock()\n",
    "audio_fade_duration = 0.3  # seconds for fade-out\n",
    "last_interrupt_time = 0\n",
    "interrupt_cooldown = 6.0  # seconds between allowed interrupts\n",
    "audio_chunk_buffer = []  # Buffer to store the most recent audio chunks for fade-out\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "model_thread = None\n",
    "model_queue = queue.Queue()\n",
    "model_result_queue = queue.Queue()\n",
    "model_thread_running = threading.Event()\n",
    "llm_lock = threading.Lock()\n",
    "audio_gen_lock = threading.Lock()\n",
    "# Database\n",
    "Base = declarative_base()\n",
    "engine = create_engine(\"sqlite:///companion.db\")\n",
    "SessionLocal = sessionmaker(bind=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10dbdb47-d295-43e1-a3a5-e3e9bc0c5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conversation(Base):\n",
    "    __tablename__ = \"conversations\"\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    session_id = Column(String, index=True)\n",
    "    timestamp = Column(String)\n",
    "    user_message = Column(Text)\n",
    "    ai_message = Column(Text)\n",
    "    audio_path = Column(String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fcadf4-6906-49bb-a060-8853846ef585",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(bind=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281c83e7-a904-4674-91a2-6f8bea5f5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic config schema\n",
    "class CompanionConfig(BaseModel):\n",
    "    system_prompt: str\n",
    "    reference_audio_path: str\n",
    "    reference_text: str\n",
    "    reference_audio_path2: Optional[str] = None  # optional field\n",
    "    reference_text2: Optional[str] = None  # optional field\n",
    "    reference_audio_path3: Optional[str] = None  # optional field\n",
    "    reference_text3: Optional[str] = None  # optional field\n",
    "    model_path: str\n",
    "    llm_path: str\n",
    "    max_tokens: int = 8192\n",
    "    voice_speaker_id: int = 0\n",
    "    vad_enabled: bool = True\n",
    "    vad_threshold: float = 0.5\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806615b3-dc2f-45be-9911-dfed6223ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state\n",
    "conversation_history = []\n",
    "config = None\n",
    "audio_queue = queue.Queue()\n",
    "is_speaking = False\n",
    "interrupt_flag = threading.Event()\n",
    "generator = None\n",
    "llm = None\n",
    "rag = None\n",
    "vad_processor = None\n",
    "reference_segments = []\n",
    "active_connections = []\n",
    "message_queue = asyncio.Queue()\n",
    "\n",
    "# Async event loop\n",
    "loop = asyncio.new_event_loop()\n",
    "asyncio.set_event_loop(loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3550503-080f-43da-b34d-71e486109c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Whisper\u001b[39;00m\n\u001b[1;32m      8\u001b[0m whisper_model \u001b[38;5;241m=\u001b[39m AutoModelForSpeechSeq2Seq\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m     model_id, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_safetensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )            \n\u001b[0;32m---> 11\u001b[0m \u001b[43mwhisper_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     13\u001b[0m whisper_pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic-speech-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mwhisper_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/transformers/modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m         )\n\u001b[0;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "# FastAPI\n",
    "app = FastAPI()\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "config_manager = ConfigManager()\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "# Whisper\n",
    "whisper_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")            \n",
    "whisper_model.to(\"cuda\")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "whisper_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=whisper_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch.float16,\n",
    "    device='cuda',\n",
    ")\n",
    "# Background queue\n",
    "async def process_message_queue():\n",
    "    while True:\n",
    "        message = await message_queue.get()\n",
    "        for client in active_connections[:]:\n",
    "            try:\n",
    "                if client.client_state == WebSocketState.CONNECTED:\n",
    "                    await client.send_json(message)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in message queue for client: {e}\")\n",
    "                if client in active_connections:\n",
    "                    active_connections.remove(client)\n",
    "        message_queue.task_done()\n",
    "\n",
    "def load_reference_segments(config_data: CompanionConfig):\n",
    "    \"\"\"Load multiple reference clips for voice‑cloning.\"\"\"\n",
    "    global reference_segments\n",
    "    reference_segments = []\n",
    "    \n",
    "    # Load primary reference (required)\n",
    "    if os.path.isfile(config_data.reference_audio_path):\n",
    "        logger.info(f\"Loading primary reference audio: {config_data.reference_audio_path}\")\n",
    "        wav, sr = torchaudio.load(config_data.reference_audio_path)\n",
    "        wav = torchaudio.functional.resample(wav.squeeze(0),\n",
    "                                         orig_freq=sr,\n",
    "                                         new_freq=24_000)\n",
    "        reference_segments.append(Segment(text=config_data.reference_text,\n",
    "                                  speaker=config_data.voice_speaker_id,\n",
    "                                  audio=wav))\n",
    "    else:\n",
    "        logger.warning(f\"Primary reference audio '{config_data.reference_audio_path}' not found.\")\n",
    "    \n",
    "    # Load second reference (optional)\n",
    "    if config_data.reference_audio_path2 and os.path.isfile(config_data.reference_audio_path2):\n",
    "        logger.info(f\"Loading second reference audio: {config_data.reference_audio_path2}\")\n",
    "        wav, sr = torchaudio.load(config_data.reference_audio_path2)\n",
    "        wav = torchaudio.functional.resample(wav.squeeze(0),\n",
    "                                         orig_freq=sr,\n",
    "                                         new_freq=24_000)\n",
    "        reference_segments.append(Segment(text=config_data.reference_text2,\n",
    "                                  speaker=config_data.voice_speaker_id,\n",
    "                                  audio=wav))\n",
    "    \n",
    "    # Load third reference (optional)\n",
    "    if config_data.reference_audio_path3 and os.path.isfile(config_data.reference_audio_path3):\n",
    "        logger.info(f\"Loading third reference audio: {config_data.reference_audio_path3}\")\n",
    "        wav, sr = torchaudio.load(config_data.reference_audio_path3)\n",
    "        wav = torchaudio.functional.resample(wav.squeeze(0),\n",
    "                                         orig_freq=sr,\n",
    "                                         new_freq=24_000)\n",
    "        reference_segments.append(Segment(text=config_data.reference_text3,\n",
    "                                  speaker=config_data.voice_speaker_id,\n",
    "                                  audio=wav))\n",
    "    \n",
    "    logger.info(f\"Loaded {len(reference_segments)} reference audio segments.\")\n",
    "\n",
    "def transcribe_audio(audio_data, sample_rate):\n",
    "    global whisper_model\n",
    "    audio_np = np.array(audio_data).astype(np.float32)\n",
    "    if sample_rate != 16000:\n",
    "        try:\n",
    "            audio_tensor = torch.tensor(audio_np).unsqueeze(0)\n",
    "            audio_tensor = torchaudio.functional.resample(audio_tensor, orig_freq=sample_rate, new_freq=16000)\n",
    "            audio_np = audio_tensor.squeeze(0).numpy()\n",
    "        except: pass\n",
    "    try:\n",
    "        with torch.jit.optimized_execution(False):\n",
    "            result = whisper_pipe(audio_np, generate_kwargs={\"language\": \"english\"}) \n",
    "            return result[\"text\"]\n",
    "    except:\n",
    "        return \"[Transcription error]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dfca611-80c9-47e9-85c1-b998dff5632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/qn3tpg6s4b1f8bz0f_70p0lr0000gn/T/ipykernel_1536/4122845572.py:942: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "/var/folders/lc/qn3tpg6s4b1f8bz0f_70p0lr0000gn/T/ipykernel_1536/4122845572.py:956: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"shutdown\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1016\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01muvicorn\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: asyncio\u001b[38;5;241m.\u001b[39mrun(loop\u001b[38;5;241m.\u001b[39mrun_forever()), daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 1016\u001b[0m \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/uvicorn/main.py:580\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    578\u001b[0m         Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m         \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# pragma: full coverage\u001b[39;00m\n",
      "File \u001b[0;32m~/csm-streaming/venvcsm.streaming/lib/python3.10/site-packages/uvicorn/server.py:67\u001b[0m, in \u001b[0;36mServer.run\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "\n",
    "def initialize_models(config_data: CompanionConfig):\n",
    "    global generator, llm, rag, vad_processor, config\n",
    "    config = config_data                         \n",
    "\n",
    "    logger.info(\"Loading LLM …\")\n",
    "    llm = LLMInterface(config_data.llm_path,\n",
    "                       config_data.max_tokens)\n",
    "\n",
    "    logger.info(\"Loading RAG …\")\n",
    "    rag = RAGSystem(\"companion.db\",\n",
    "                    model_name=config_data.embedding_model)\n",
    "\n",
    "    vad_model, vad_utils = torch.hub.load('snakers4/silero-vad',\n",
    "                                          model='silero_vad',\n",
    "                                          force_reload=False)\n",
    "    vad_processor = AudioStreamProcessor(\n",
    "        model=vad_model,\n",
    "        utils=vad_utils,\n",
    "        sample_rate=16_000,\n",
    "        vad_threshold=config_data.vad_threshold,\n",
    "        callbacks={\"on_speech_start\": on_speech_start,\n",
    "                   \"on_speech_end\":   on_speech_end},\n",
    "    )\n",
    "\n",
    "    load_reference_segments(config_data)\n",
    "\n",
    "    start_model_thread()\n",
    "\n",
    "    logger.info(\"Compiling / warming‑up voice model …\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # send a dummy request; max 0.5 s of audio, result discarded\n",
    "    model_queue.put((\n",
    "        \"warm‑up.\",                          # text\n",
    "        config_data.voice_speaker_id,        # speaker\n",
    "        [],                                  # no context\n",
    "        500,                                 # max_ms\n",
    "        0.7,                                 # temperature\n",
    "        40,                                  # top‑k\n",
    "    ))\n",
    "\n",
    "    # block until worker signals EOS (None marker)\n",
    "    while True:\n",
    "        r = model_result_queue.get()\n",
    "        if r is None:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"Voice model ready in {time.time() - t0:.1f}s\")\n",
    "\n",
    "\n",
    "def on_speech_start():\n",
    "    asyncio.run_coroutine_threadsafe(\n",
    "        message_queue.put(\n",
    "            {\n",
    "                \"type\":   \"vad_status\",\n",
    "                \"status\": \"speech_started\",\n",
    "                \"should_interrupt\": False,  # always False – UI never barges-in here\n",
    "            }\n",
    "        ),\n",
    "        loop,\n",
    "    )\n",
    "\n",
    "def on_speech_end(audio_data, sample_rate):\n",
    "    try:\n",
    "        logger.info(\"Transcription starting\")\n",
    "        user_text = transcribe_audio(audio_data, sample_rate)\n",
    "        logger.info(f\"Transcription completed: '{user_text}'\")\n",
    "\n",
    "        session_id = \"default\"\n",
    "        speaker_id = 1\n",
    "        index = speaker_counters[speaker_id]\n",
    "        user_audio_path = f\"audio/user/{session_id}_user_{index}.wav\"\n",
    "        os.makedirs(os.path.dirname(user_audio_path), exist_ok=True)\n",
    "\n",
    "        audio_tensor = torch.tensor(audio_data).unsqueeze(0)\n",
    "        save_audio_and_trim(user_audio_path, session_id, speaker_id, audio_tensor.squeeze(0), sample_rate)\n",
    "        add_segment(user_text, speaker_id, audio_tensor.squeeze(0))\n",
    "\n",
    "        logger.info(f\"User audio saved and segment appended: {user_audio_path}\")\n",
    "\n",
    "        speaker_counters[speaker_id] += 1\n",
    "\n",
    "        # Send transcription to clients\n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "            message_queue.put({\"type\": \"transcription\", \"text\": user_text}),\n",
    "            loop\n",
    "        )\n",
    "        \n",
    "        threading.Thread(target=lambda: process_user_input(user_text, session_id), daemon=True).start()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"VAD callback failed: {e}\")\n",
    "\n",
    "def process_pending_inputs():\n",
    "    \"\"\"Process only the latest user input after an interruption\"\"\"\n",
    "    global pending_user_inputs, is_speaking, interrupt_flag\n",
    "    time.sleep(0.2)\n",
    "    is_speaking = False\n",
    "    interrupt_flag.clear()\n",
    "    \n",
    "    with user_input_lock:\n",
    "        if not pending_user_inputs:\n",
    "            logger.info(\"No pending user inputs to process\")\n",
    "            return\n",
    "        \n",
    "        # Only take the most recent input and ignore others\n",
    "        latest_input = pending_user_inputs[-1]\n",
    "        logger.info(f\"Processing only latest input: '{latest_input[0]}'\")\n",
    "        \n",
    "        # Clear all pending inputs\n",
    "        pending_user_inputs = []\n",
    "        \n",
    "        # Process only the latest input\n",
    "        user_text, session_id = latest_input\n",
    "        process_user_input(user_text, session_id)\n",
    "\n",
    "def process_user_input(user_text, session_id=\"default\"):\n",
    "    global config, is_speaking, pending_user_inputs, interrupt_flag\n",
    "    \n",
    "    # Skip empty messages\n",
    "    if not user_text or user_text.strip() == \"\":\n",
    "        logger.warning(\"Empty user input received, ignoring\")\n",
    "        return\n",
    "    \n",
    "    interrupt_flag.clear()\n",
    "    is_speaking = False\n",
    "    \n",
    "    # Check if we're currently supposed to be speaking\n",
    "    if is_speaking:\n",
    "        logger.info(f\"AI is currently speaking, adding input to pending queue: '{user_text}'\")\n",
    "        \n",
    "        with user_input_lock:\n",
    "            # Only keep the most recent input, replacing any existing ones\n",
    "            pending_user_inputs = [(user_text, session_id)]\n",
    "            logger.info(f\"Added user input as the only pending input: '{user_text}'\")\n",
    "        \n",
    "        # Request interruption if not already interrupted\n",
    "        if not interrupt_flag.is_set():\n",
    "            logger.info(\"Automatically interrupting current speech for new input\")\n",
    "            interrupt_flag.set()\n",
    "            # Notify clients of interruption\n",
    "            asyncio.run_coroutine_threadsafe(\n",
    "                message_queue.put({\"type\": \"audio_status\", \"status\": \"interrupted\"}),\n",
    "                loop\n",
    "            )\n",
    "            \n",
    "            # Allow a short delay before processing the new input\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            # Process the pending input after interruption\n",
    "            process_pending_inputs()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    interrupt_flag.clear()\n",
    "    \n",
    "    # Normal processing continues...\n",
    "    logger.info(f\"Processing user input: '{user_text}'\")\n",
    "    context = \"\\n\".join([f\"User: {msg['user']}\\nAI: {msg['ai']}\" for msg in conversation_history[-5:]])\n",
    "    rag_context = rag.query(user_text)\n",
    "    system_prompt = config.system_prompt\n",
    "    if rag_context:\n",
    "        system_prompt += f\"\\n\\nRelevant context:\\n{rag_context}\"\n",
    "\n",
    "    # Notify clients that we're thinking\n",
    "    asyncio.run_coroutine_threadsafe(\n",
    "        message_queue.put({\"type\": \"status\", \"message\": \"Thinking...\"}),\n",
    "        loop\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with llm_lock: \n",
    "            ai_response = llm.generate_response(system_prompt, user_text, context)\n",
    "        \n",
    "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        conversation_history.append({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"user\": user_text,\n",
    "            \"ai\": ai_response\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            db = SessionLocal()\n",
    "            conv = Conversation(\n",
    "                session_id=session_id,\n",
    "                timestamp=timestamp,\n",
    "                user_message=user_text,\n",
    "                ai_message=ai_response,\n",
    "                audio_path=\"\"\n",
    "            )\n",
    "            db.add(conv)\n",
    "            db.commit()\n",
    "            index = speaker_counters[0]\n",
    "            output_file = f\"audio/ai/{session_id}_response_{index}.wav\"\n",
    "            speaker_counters[0] += 1\n",
    "            conv.audio_path = output_file\n",
    "            db.commit()\n",
    "            db.close()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Database error: {e}\")\n",
    "        \n",
    "        threading.Thread(target=lambda: rag.add_conversation(user_text, ai_response), daemon=True).start()\n",
    "        \n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "            message_queue.put({\"type\": \"audio_status\", \"status\": \"preparing\"}),\n",
    "            loop\n",
    "        )\n",
    "        \n",
    "        # Small delay to ensure client is ready\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        # Send the response to clients\n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "            message_queue.put({\"type\": \"response\", \"text\": ai_response}),\n",
    "            loop\n",
    "        )\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if is_speaking:\n",
    "            logger.warning(\"Still speaking when trying to start new audio - forcing interrupt\")\n",
    "            interrupt_flag.set()\n",
    "            is_speaking = False\n",
    "            time.sleep(0.5)  # Give time for cleanup\n",
    "        \n",
    "        interrupt_flag.clear()  # Make absolutely sure\n",
    "        is_speaking = False    # Reset for audio thread to take over\n",
    "        \n",
    "        # Start audio generation in a new thread\n",
    "        threading.Thread(target=audio_generation_thread, args=(ai_response, output_file), daemon=True).start()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating response: {e}\")\n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "            message_queue.put({\"type\": \"error\", \"message\": \"Failed to generate response\"}),\n",
    "            loop\n",
    "        )\n",
    "\n",
    "def model_worker(cfg: CompanionConfig):\n",
    "    global generator, model_thread_running\n",
    "\n",
    "    logger.info(\"Model worker thread started\")\n",
    "\n",
    "    if generator is None:\n",
    "        torch._inductor.config.triton.cudagraphs = False  # Disable cudagraphs\n",
    "        torch._inductor.config.fx_graph_cache = False  # Disable graph caching\n",
    "        logger.info(\"Loading voice model inside worker thread …\")\n",
    "        generator = load_csm_1b_local(cfg.model_path, \"cuda\")\n",
    "        logger.info(\"Voice model ready (compiled with cudagraphs)\")\n",
    "\n",
    "    while model_thread_running.is_set():\n",
    "        try:\n",
    "            request = model_queue.get(timeout=0.1)\n",
    "            if request is None:\n",
    "                break\n",
    "\n",
    "            text, speaker_id, context, max_ms, temperature, topk = request\n",
    "\n",
    "            for chunk in generator.generate_stream(\n",
    "                    text=text,\n",
    "                    speaker=speaker_id,\n",
    "                    context=context,\n",
    "                    max_audio_length_ms=max_ms,\n",
    "                    temperature=temperature,\n",
    "                    topk=topk):\n",
    "                model_result_queue.put(chunk)\n",
    "\n",
    "                if not model_thread_running.is_set():\n",
    "                    break\n",
    "\n",
    "            model_result_queue.put(None) # EOS marker\n",
    "\n",
    "        except queue.Empty:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            logger.error(f\"Error in model worker: {e}\\n{traceback.format_exc()}\")\n",
    "            model_result_queue.put(Exception(f\"Generation error: {e}\"))\n",
    "\n",
    "    logger.info(\"Model worker thread exiting\")\n",
    "\n",
    "def start_model_thread():\n",
    "    global model_thread, model_thread_running\n",
    "\n",
    "    if model_thread is not None and model_thread.is_alive():\n",
    "        return                        \n",
    "\n",
    "    model_thread_running.set()\n",
    "    model_thread = threading.Thread(target=model_worker,\n",
    "                                    args=(config,),\n",
    "                                    daemon=True,\n",
    "                                    name=\"model_worker\")\n",
    "    model_thread.start()\n",
    "    logger.info(\"Started dedicated model worker thread\")\n",
    "\n",
    "async def run_audio_generation(text, output_file):\n",
    "    \"\"\"Async wrapper for audio generation that runs in the event loop thread\"\"\"\n",
    "    audio_generation_thread(text, output_file)\n",
    "\n",
    "def send_to_all_clients(message: dict):\n",
    "    \"\"\"Send a message to all connected WebSocket clients\"\"\"\n",
    "    for client in active_connections[:]:\n",
    "        try:\n",
    "            if client.client_state == WebSocketState.CONNECTED:\n",
    "                asyncio.run_coroutine_threadsafe(client.send_json(message), loop)\n",
    "                logger.info(f\"Sent message to client: {message}\")\n",
    "            else:\n",
    "                logger.warning(\"Detected non-connected client; removing from active_connections\")\n",
    "                active_connections.remove(client)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error sending message to client: {e}\")\n",
    "            if client in active_connections:\n",
    "                active_connections.remove(client)\n",
    "\n",
    "saved_audio_paths = {\n",
    "    \"default\": {\n",
    "        0: [],  # AI\n",
    "        1: []   # User\n",
    "    }\n",
    "}\n",
    "MAX_AUDIO_FILES = 8\n",
    "\n",
    "def save_audio_and_trim(path, session_id, speaker_id, tensor, sample_rate):\n",
    "    \"\"\"\n",
    "    Save audio file and trim old audio files for both AI and user to maintain storage limits.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to save the audio file\n",
    "        session_id: Conversation session ID\n",
    "        speaker_id: 0 for AI, 1 for user\n",
    "        tensor: Audio tensor to save\n",
    "        sample_rate: Audio sample rate\n",
    "    \"\"\"\n",
    "    torchaudio.save(path, tensor.unsqueeze(0), sample_rate)\n",
    "    \n",
    "    saved_audio_paths.setdefault(session_id, {}).setdefault(speaker_id, []).append(path)\n",
    "    \n",
    "    paths = saved_audio_paths[session_id][speaker_id]\n",
    "    while len(paths) > MAX_AUDIO_FILES:\n",
    "        old_path = paths.pop(0)\n",
    "        if os.path.exists(old_path):\n",
    "            os.remove(old_path)\n",
    "            logger.info(f\"Removed old audio file: {old_path}\")\n",
    "    \n",
    "    other_speaker_id = 1 if speaker_id == 0 else 0\n",
    "    if other_speaker_id in saved_audio_paths[session_id]:\n",
    "        other_paths = saved_audio_paths[session_id][other_speaker_id]\n",
    "        while len(other_paths) > MAX_AUDIO_FILES:\n",
    "            old_path = other_paths.pop(0)\n",
    "            if os.path.exists(old_path):\n",
    "                os.remove(old_path)\n",
    "                logger.info(f\"Removed old audio file from other speaker: {old_path}\")\n",
    "\n",
    "MAX_SEGMENTS = 8\n",
    "\n",
    "def add_segment(text, speaker_id, audio_tensor):\n",
    "    \"\"\"\n",
    "    Add a new segment and ensure the total context stays within token limits.\n",
    "    Preserves the original reference segments when trimming.\n",
    "    \n",
    "    Args:\n",
    "        text: Text content of the segment\n",
    "        speaker_id: ID of the speaker (0 for AI, 1 for user)\n",
    "        audio_tensor: Audio data as a tensor\n",
    "    \"\"\"\n",
    "    global reference_segments, generator, config\n",
    "    \n",
    "    # Count how many original reference segments we have (1-3)\n",
    "    num_reference_segments = 1  # We always have at least the primary reference\n",
    "    if hasattr(config, 'reference_audio_path2') and config.reference_audio_path2:\n",
    "        num_reference_segments += 1\n",
    "    if hasattr(config, 'reference_audio_path3') and config.reference_audio_path3:\n",
    "        num_reference_segments += 1\n",
    "    \n",
    "    # Add the new segment\n",
    "    new_segment = Segment(text=text, speaker=speaker_id, audio=audio_tensor)\n",
    "    \n",
    "    # Keep original reference segments protected from trimming\n",
    "    protected_segments = reference_segments[:num_reference_segments] if len(reference_segments) >= num_reference_segments else reference_segments.copy()\n",
    "    \n",
    "    # Dynamic segments that can be trimmed\n",
    "    dynamic_segments = reference_segments[num_reference_segments:] if len(reference_segments) > num_reference_segments else []\n",
    "    dynamic_segments.append(new_segment)\n",
    "    \n",
    "    # First trim by MAX_SEGMENTS if needed, but never trim protected segments\n",
    "    while len(protected_segments) + len(dynamic_segments) > MAX_SEGMENTS:\n",
    "        if dynamic_segments:\n",
    "            dynamic_segments.pop(0)  # Remove the oldest non-protected segment\n",
    "        else:\n",
    "            break  # Safety check - shouldn't happen\n",
    "    \n",
    "    # Combine protected and dynamic segments\n",
    "    reference_segments = protected_segments + dynamic_segments\n",
    "    \n",
    "    # Then check and trim by token count\n",
    "    # We need to access the model's tokenizer to properly count tokens\n",
    "    if hasattr(generator, '_text_tokenizer'):\n",
    "        total_tokens = 0\n",
    "        \n",
    "        # Count tokens in all segments\n",
    "        for segment in reference_segments:\n",
    "            tokens = generator._text_tokenizer.encode(f\"[{segment.speaker}]{segment.text}\")\n",
    "            total_tokens += len(tokens)\n",
    "            if segment.audio is not None:\n",
    "                audio_frames = segment.audio.size(0) // 285  # Approximate frame count\n",
    "                total_tokens += audio_frames\n",
    "        \n",
    "        # Remove oldest dynamic segments until we're under the token limit\n",
    "        # but never remove protected segments\n",
    "        while dynamic_segments and total_tokens > 2048:\n",
    "            removed = dynamic_segments.pop(0)\n",
    "            reference_segments.remove(removed)\n",
    "            \n",
    "            # Recalculate tokens for the removed segment\n",
    "            removed_tokens = len(generator._text_tokenizer.encode(f\"[{removed.speaker}]{removed.text}\"))\n",
    "            if removed.audio is not None:\n",
    "                removed_audio_frames = removed.audio.size(0) // 285\n",
    "                removed_tokens += removed_audio_frames\n",
    "            total_tokens -= removed_tokens\n",
    "            \n",
    "        logger.info(f\"Segments: {len(reference_segments)} \" +\n",
    "                    f\"({len(protected_segments)} protected, {len(dynamic_segments)} dynamic), \" +\n",
    "                    f\"total tokens: {total_tokens}/2048\")\n",
    "    else:\n",
    "        # Fallback if we can't access the tokenizer - make a rough estimate\n",
    "        logger.warning(\"Unable to access tokenizer - falling back to word-based estimation\")\n",
    "        \n",
    "        def estimate_tokens(segment):\n",
    "            # Rough token estimation based on words and punctuation\n",
    "            words = segment.text.split()\n",
    "            punctuation = sum(1 for char in segment.text if char in \".,!?;:\\\"'()[]{}\")\n",
    "            text_tokens = len(words) + punctuation\n",
    "            \n",
    "            # Estimate audio tokens\n",
    "            audio_tokens = 0\n",
    "            if segment.audio is not None:\n",
    "                audio_frames = segment.audio.size(0) // 300  # Approximate frame count\n",
    "                audio_tokens = audio_frames\n",
    "                \n",
    "            return text_tokens + audio_tokens\n",
    "        \n",
    "        # Calculate total token count\n",
    "        total_estimated_tokens = sum(estimate_tokens(segment) for segment in reference_segments)\n",
    "        \n",
    "        # Remove oldest dynamic segments until we're under the token limit\n",
    "        while dynamic_segments and total_estimated_tokens > 2048:\n",
    "            removed = dynamic_segments.pop(0)\n",
    "            idx = reference_segments.index(removed)\n",
    "            reference_segments.pop(idx)\n",
    "            total_estimated_tokens -= estimate_tokens(removed)\n",
    "            \n",
    "        logger.info(f\"Segments: {len(reference_segments)} \" +\n",
    "                    f\"({len(protected_segments)} protected, {len(dynamic_segments)} dynamic), \" +\n",
    "                    f\"estimated tokens: {total_estimated_tokens}/2048\")\n",
    "\n",
    "def preprocess_text_for_tts(text):\n",
    "    \"\"\"\n",
    "    Removes all punctuation except periods, commas, exclamation points, and question marks\n",
    "    from the input text to create cleaner speech output while preserving intonation.\n",
    "    Args:\n",
    "    text (str): Input text with potential punctuation\n",
    "    Returns:\n",
    "    str: Cleaned text with only allowed punctuation\n",
    "    \"\"\"\n",
    "    # Define a regex pattern that matches all punctuation except periods, commas, exclamation points, and question marks\n",
    "    # This includes: ; : \" '  ~ @ # $ % ^ & * ( ) _ - + = [ ] { } \\ | / < >\n",
    "    pattern = r'[^\\w\\s.,!?\\']'\n",
    "    # Replace matched punctuation with empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    # normalize multiple spaces to single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    # ensure there's a space after punctuation for better speech pacing\n",
    "    cleaned_text = re.sub(r'([.,!?])(\\S)', r'\\1 \\2', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def audio_generation_thread(text, output_file):\n",
    "    global is_speaking, interrupt_flag, audio_queue, model_thread_running, current_generation_id, speaking_start_time\n",
    "    \n",
    "    current_generation_id += 1\n",
    "    this_id = current_generation_id\n",
    "    \n",
    "    interrupt_flag.clear()\n",
    "    \n",
    "    # Log the start of generation\n",
    "    logger.info(f\"Starting audio generation for ID: {this_id}\")\n",
    "    \n",
    "    # Try to acquire the lock, but don't block if it's busy\n",
    "    if not audio_gen_lock.acquire(blocking=False):\n",
    "        logger.warning(f\"Audio generation {this_id} - lock acquisition failed, another generation is in progress\")\n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "            message_queue.put({\n",
    "                \"type\": \"error\", \n",
    "                \"message\": \"Audio generation busy, skipping synthesis\",\n",
    "                \"gen_id\": this_id\n",
    "            }),\n",
    "            loop\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Start the model thread if it's not already running\n",
    "        start_model_thread()\n",
    "        \n",
    "        interrupt_flag.clear()\n",
    "        is_speaking = True\n",
    "        speaking_start_time = time.time()\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        all_audio_chunks = []\n",
    "        \n",
    "        # Prepare text\n",
    "        text_lower = text.lower()\n",
    "        text_lower = preprocess_text_for_tts(text_lower)\n",
    "        \n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "            message_queue.put({\n",
    "                \"type\": \"audio_status\", \n",
    "                \"status\": \"preparing_generation\",\n",
    "                \"gen_id\": this_id\n",
    "            }),\n",
    "            loop\n",
    "        )\n",
    "        \n",
    "        # Give client a moment to process\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        logger.info(f\"Sending generating status with ID {this_id}\")\n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "            message_queue.put({\n",
    "                \"type\": \"audio_status\", \n",
    "                \"status\": \"generating\",\n",
    "                \"gen_id\": this_id  # Include generation ID\n",
    "            }),\n",
    "            loop\n",
    "        )\n",
    "        \n",
    "        # Small delay to ensure client gets the signal\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        # Estimate audio length\n",
    "        words = text.split()\n",
    "        avg_wpm = 100\n",
    "        words_per_second = avg_wpm / 60\n",
    "        estimated_seconds = len(words) / words_per_second\n",
    "        max_audio_length_ms = int(estimated_seconds * 1000)\n",
    "        \n",
    "        # Send request to model thread\n",
    "        logger.info(f\"Audio generation {this_id} - sending request to model thread\")\n",
    "        model_queue.put((\n",
    "            text_lower,\n",
    "            config.voice_speaker_id,\n",
    "            reference_segments,\n",
    "            max_audio_length_ms,\n",
    "            0.8,  # temperature\n",
    "            50    # topk\n",
    "        ))\n",
    "        \n",
    "        # Start timing\n",
    "        generation_start = time.time()\n",
    "        chunk_counter = 0\n",
    "        \n",
    "        # Process results as they come\n",
    "        while True:\n",
    "            try:\n",
    "                # Check for interruption FIRST before getting more results\n",
    "                if interrupt_flag.is_set():\n",
    "                    logger.info(f\"Audio generation {this_id} - interrupt detected, stopping\")\n",
    "                    \n",
    "                    # Signal model thread to exit and restart\n",
    "                    model_thread_running.clear()\n",
    "                    time.sleep(0.1)\n",
    "                    model_thread_running.set()\n",
    "                    start_model_thread()\n",
    "                    \n",
    "                    # Clear any remaining items in the result queue\n",
    "                    while not model_result_queue.empty():\n",
    "                        try:\n",
    "                            model_result_queue.get_nowait()\n",
    "                        except queue.Empty:\n",
    "                            pass\n",
    "                    \n",
    "                    # Break out of the processing loop\n",
    "                    break\n",
    "                \n",
    "                # Get result with timeout to allow checking interrupt\n",
    "                result = model_result_queue.get(timeout=0.1)\n",
    "                \n",
    "                # Check for end of generation or error\n",
    "                if result is None:\n",
    "                    logger.info(f\"Audio generation {this_id} - complete\")\n",
    "                    break\n",
    "                    \n",
    "                if isinstance(result, Exception):\n",
    "                    logger.error(f\"Audio generation {this_id} - error: {result}\")\n",
    "                    raise result\n",
    "                \n",
    "                # Track timing for first chunk\n",
    "                if chunk_counter == 0:\n",
    "                    first_chunk_time = time.time() - generation_start\n",
    "                    logger.info(f\"Audio generation {this_id} - first chunk latency: {first_chunk_time*1000:.1f}ms\")\n",
    "                \n",
    "                chunk_counter += 1\n",
    "                \n",
    "                # One more interrupt check before processing chunk\n",
    "                if interrupt_flag.is_set():\n",
    "                    logger.info(f\"Audio generation {this_id} - interrupt flag set during chunk processing\")\n",
    "                    break\n",
    "                \n",
    "                # Process this audio chunk\n",
    "                audio_chunk = result\n",
    "                all_audio_chunks.append(audio_chunk)\n",
    "                \n",
    "                # Convert to numpy and send to audio queue\n",
    "                chunk_array = audio_chunk.cpu().numpy().astype(np.float32)\n",
    "                audio_queue.put(chunk_array)\n",
    "                \n",
    "                if chunk_counter == 1:\n",
    "                    logger.info(f\"Sending first audio chunk with ID {this_id}\")\n",
    "                    # Notify client we're sending the first chunk\n",
    "                    asyncio.run_coroutine_threadsafe(\n",
    "                        message_queue.put({\n",
    "                            \"type\": \"audio_status\", \n",
    "                            \"status\": \"first_chunk\",\n",
    "                            \"gen_id\": this_id\n",
    "                        }),\n",
    "                        loop\n",
    "                    )\n",
    "                    # Small delay\n",
    "                    time.sleep(0.1)\n",
    "                \n",
    "                # Send chunk with generation ID\n",
    "                asyncio.run_coroutine_threadsafe(\n",
    "                    message_queue.put({\n",
    "                        \"type\": \"audio_chunk\",\n",
    "                        \"audio\": chunk_array.tolist(),\n",
    "                        \"sample_rate\": generator.sample_rate,\n",
    "                        \"gen_id\": this_id,\n",
    "                        \"chunk_num\": chunk_counter  # Include chunk number\n",
    "                    }),\n",
    "                    loop\n",
    "                )\n",
    "                \n",
    "            except queue.Empty:\n",
    "                # No results yet, keep checking\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Audio generation {this_id} - error processing result: {e}\")\n",
    "                break\n",
    "        \n",
    "        # Save complete audio if available\n",
    "        if all_audio_chunks and not interrupt_flag.is_set():\n",
    "            try:\n",
    "                complete_audio = torch.cat(all_audio_chunks)\n",
    "                save_audio_and_trim(output_file, \"default\", config.voice_speaker_id, complete_audio, generator.sample_rate)\n",
    "                add_segment(text.lower(), config.voice_speaker_id, complete_audio)\n",
    "                \n",
    "                # Log statistics\n",
    "                total_time = time.time() - generation_start\n",
    "                total_audio_seconds = complete_audio.size(0) / generator.sample_rate\n",
    "                rtf = total_time / total_audio_seconds\n",
    "                logger.info(f\"Audio generation {this_id} - completed in {total_time:.2f}s, RTF: {rtf:.2f}x\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Audio generation {this_id} - error saving complete audio: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        logger.error(f\"Audio generation {this_id} - unexpected error: {e}\\n{traceback.format_exc()}\")\n",
    "    finally:\n",
    "        is_speaking = False\n",
    "        \n",
    "        # Signal end of audio\n",
    "        audio_queue.put(None)\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Audio generation {this_id} - sending completion status\")\n",
    "            asyncio.run_coroutine_threadsafe(\n",
    "                message_queue.put({\n",
    "                    \"type\": \"audio_status\", \n",
    "                    \"status\": \"complete\",\n",
    "                    \"gen_id\": this_id\n",
    "                }),\n",
    "                loop\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Audio generation {this_id} - failed to send completion status: {e}\")\n",
    "            \n",
    "        # Process any pending inputs\n",
    "        with user_input_lock:\n",
    "            if pending_user_inputs:\n",
    "                # Process pending inputs\n",
    "                logger.info(f\"Audio generation {this_id} - processing pending inputs\")\n",
    "                process_pending_inputs()\n",
    "            \n",
    "        # Release the lock\n",
    "        logger.info(f\"Audio generation {this_id} - releasing lock\")\n",
    "        audio_gen_lock.release()\n",
    "    \n",
    "def handle_interrupt(websocket):\n",
    "    global is_speaking, last_interrupt_time, interrupt_flag, model_thread_running, speaking_start_time\n",
    "    \n",
    "    # Log the current state\n",
    "    logger.info(f\"Interrupt requested. Current state: is_speaking={is_speaking}\")\n",
    "    \n",
    "    current_time = time.time()\n",
    "    time_since_speech_start = current_time - speaking_start_time if speaking_start_time > 0 else 999\n",
    "    time_since_last_interrupt = current_time - last_interrupt_time\n",
    "    \n",
    "    # Only apply cooldown for established speech, not for new speech\n",
    "    if time_since_last_interrupt < interrupt_cooldown and time_since_speech_start > 3.0:\n",
    "        logger.info(f\"Ignoring interrupt: too soon after previous interrupt ({time_since_last_interrupt:.1f}s < {interrupt_cooldown}s)\")\n",
    "        # Let the client know we're not interrupting\n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "           websocket.send_json({\n",
    "               \"type\": \"audio_status\",\n",
    "               \"status\": \"interrupt_acknowledged\",\n",
    "               \"success\": False,\n",
    "               \"reason\": \"cooldown\"\n",
    "           }),\n",
    "           loop\n",
    "        )\n",
    "        return False\n",
    "    \n",
    "    # Update the last interrupt time\n",
    "    last_interrupt_time = current_time\n",
    "    \n",
    "    # We should interrupt if we're speaking OR if model generation is in progress\n",
    "    if is_speaking or not model_result_queue.empty():\n",
    "        logger.info(\"Interruption processing: we are speaking or generating\")\n",
    "        \n",
    "        interrupt_flag.set()\n",
    "        \n",
    "        # Notify clients\n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "            message_queue.put({\"type\": \"audio_status\", \"status\": \"interrupted\"}),\n",
    "            loop\n",
    "        )\n",
    "        \n",
    "        asyncio.run_coroutine_threadsafe(\n",
    "           websocket.send_json({\n",
    "               \"type\": \"audio_status\",\n",
    "               \"status\": \"interrupt_acknowledged\"\n",
    "           }),\n",
    "           loop\n",
    "        )\n",
    "        \n",
    "        # Clear the audio queue to stop additional audio from being processed\n",
    "        try:\n",
    "            # Drain the existing queue\n",
    "            while not audio_queue.empty():\n",
    "                try:\n",
    "                    audio_queue.get_nowait()\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "                    \n",
    "            # Add end signal\n",
    "            audio_queue.put(None)\n",
    "            logger.info(\"Audio queue cleared\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error clearing audio queue: {e}\")\n",
    "        \n",
    "        # Reset VAD to prepare for new input\n",
    "        if vad_processor:\n",
    "            try:\n",
    "                vad_processor.reset()\n",
    "                logger.info(\"VAD processor reset\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error resetting VAD: {e}\")\n",
    "        \n",
    "        # Stop current model worker if needed\n",
    "        if model_thread and model_thread.is_alive():\n",
    "            try:\n",
    "                # Clear the thread running flag to stop generation\n",
    "                model_thread_running.clear()\n",
    "                \n",
    "                # Wait a brief moment for thread to notice and exit\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "                # Now restart the thread state flag\n",
    "                model_thread_running.set()\n",
    "                \n",
    "                # And restart the thread\n",
    "                start_model_thread()\n",
    "                logger.info(\"Model thread restarted\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error restarting model thread: {e}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    logger.info(\"No active speech to interrupt\")\n",
    "    return False\n",
    "\n",
    "@app.websocket(\"/ws\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    global is_speaking, audio_queue\n",
    "    \n",
    "    await websocket.accept()\n",
    "    active_connections.append(websocket)\n",
    "    \n",
    "    saved = config_manager.load_config()\n",
    "    if saved:\n",
    "        await websocket.send_json({\"type\": \"saved_config\", \"config\": saved})\n",
    "        \n",
    "    try:\n",
    "        while True:\n",
    "            data = await websocket.receive_json()\n",
    "            \n",
    "            if data[\"type\"] == \"config\":\n",
    "                # Config handling\n",
    "                try:\n",
    "                    config_data = data[\"config\"]\n",
    "                    \n",
    "                    logger.info(f\"Received config data keys: {config_data.keys()}\")\n",
    "\n",
    "                    for key in [\"reference_audio_path\", \"reference_audio_path2\", \"reference_audio_path3\",\n",
    "                               \"reference_text\", \"reference_text2\", \"reference_text3\"]:\n",
    "                        if key in config_data:\n",
    "                            logger.info(f\"Config includes {key}: {config_data[key]}\")\n",
    "                        else:\n",
    "                            logger.warning(f\"Config missing {key}\")\n",
    "                    \n",
    "                    conf = CompanionConfig(**config_data)\n",
    "                    \n",
    "                    saved = config_manager.save_config(config_data)\n",
    "                    \n",
    "                    if saved:\n",
    "                        initialize_models(conf)\n",
    "                        await websocket.send_json({\"type\": \"status\", \"message\": \"Models initialized and configuration saved\"})\n",
    "                    else:\n",
    "                        await websocket.send_json({\"type\": \"error\", \"message\": \"Failed to save configuration\"})\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing config: {str(e)}\")\n",
    "                    await websocket.send_json({\"type\": \"error\", \"message\": f\"Configuration error: {str(e)}\"})\n",
    "                \n",
    "                \n",
    "            elif data[\"type\"] == \"request_saved_config\":\n",
    "                saved = config_manager.load_config()\n",
    "                await websocket.send_json({\"type\": \"saved_config\", \"config\": saved})\n",
    "            \n",
    "            elif data[\"type\"] == \"text_message\":\n",
    "                user_text   = data[\"text\"]\n",
    "                session_id  = data.get(\"session_id\", \"default\")\n",
    "                logger.info(f\"TEXT-MSG from client: {user_text!r}\")\n",
    "\n",
    "                # If the model is already talking, queue the request but\n",
    "                if is_speaking:\n",
    "                    with user_input_lock:\n",
    "                        if len(pending_user_inputs) >= 3:\n",
    "                            pending_user_inputs = pending_user_inputs[-2:]\n",
    "                        pending_user_inputs.append((user_text, session_id))\n",
    "                    await websocket.send_json(\n",
    "                        {\"type\":\"status\",\"message\":\"Queued – I’ll answer in a moment\"})\n",
    "                    continue                         \n",
    "\n",
    "                await message_queue.put({\"type\":\"transcription\",\"text\":user_text})\n",
    "                threading.Thread(\n",
    "                    target=lambda: process_user_input(user_text, session_id),\n",
    "                    daemon=True).start()\n",
    "                \n",
    "            elif data[\"type\"] == \"audio\":\n",
    "                audio_data = np.asarray(data[\"audio\"], dtype=np.float32)\n",
    "                sample_rate = data[\"sample_rate\"]\n",
    "\n",
    "                if sample_rate != 16000:\n",
    "                    audio_tensor = torch.tensor(audio_data).unsqueeze(0)\n",
    "                    audio_tensor = torchaudio.functional.resample(\n",
    "                        audio_tensor, orig_freq=sample_rate, new_freq=16000\n",
    "                    )\n",
    "                    audio_data  = audio_tensor.squeeze(0).numpy()\n",
    "                    sample_rate = 16000\n",
    "\n",
    "                if config and config.vad_enabled:\n",
    "                    vad_processor.process_audio(audio_data)  \n",
    "                else:\n",
    "                    text = transcribe_audio(audio_data, sample_rate)\n",
    "                    await websocket.send_json({\"type\": \"transcription\", \"text\": text})\n",
    "                    await message_queue.put({\"type\": \"transcription\", \"text\": text})\n",
    "\n",
    "                    if is_speaking:\n",
    "                        with user_input_lock:\n",
    "                            pending_user_inputs.append((text, \"default\"))\n",
    "                    else:\n",
    "                        process_user_input(text)\n",
    "\n",
    "                        \n",
    "            elif data[\"type\"] == \"interrupt\":\n",
    "                logger.info(\"Explicit interrupt request received\")\n",
    "                \n",
    "                # Always acknowledge receipt of interrupt request\n",
    "                await websocket.send_json({\n",
    "                    \"type\": \"audio_status\", \n",
    "                    \"status\": \"interrupt_acknowledged\"\n",
    "                })\n",
    "                \n",
    "                # Then try to handle the actual interrupt\n",
    "                success = handle_interrupt(websocket)\n",
    "                \n",
    "                # If successful, allow a brief delay for clearing everything\n",
    "                if success:\n",
    "                    await asyncio.sleep(0.3)  # Short delay to allow complete clearing\n",
    "                    \n",
    "                    # Force process pending inputs after interrupt\n",
    "                    with user_input_lock:\n",
    "                        if pending_user_inputs:\n",
    "                            user_text, session_id = pending_user_inputs.pop(0)\n",
    "                            pending_user_inputs.clear()  # Clear any backup to avoid multiple responses\n",
    "                            \n",
    "                            # Process in a new thread to avoid blocking\n",
    "                            threading.Thread(\n",
    "                                target=lambda: process_user_input(user_text, session_id),\n",
    "                                daemon=True\n",
    "                            ).start()\n",
    "                \n",
    "                # Send final status update about the interrupt\n",
    "                await websocket.send_json({\n",
    "                    \"type\": \"audio_status\", \n",
    "                    \"status\": \"interrupted\",\n",
    "                    \"success\": success\n",
    "                })\n",
    "                \n",
    "            elif data[\"type\"] == \"mute\":\n",
    "                await websocket.send_json({\"type\": \"mute_status\", \"muted\": data[\"muted\"]})\n",
    "                if not data[\"muted\"] and config and config.vad_enabled:\n",
    "                    vad_processor.reset()\n",
    "                    \n",
    "    except WebSocketDisconnect:\n",
    "        if websocket in active_connections:\n",
    "            active_connections.remove(websocket)\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def index(request: Request):\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
    "\n",
    "@app.get(\"/setup\", response_class=HTMLResponse)\n",
    "async def setup_page(request: Request):\n",
    "    return templates.TemplateResponse(\"setup.html\", {\"request\": request})\n",
    "\n",
    "@app.get(\"/chat\", response_class=HTMLResponse)\n",
    "async def chat_page(request: Request):\n",
    "    return templates.TemplateResponse(\"chat.html\", {\"request\": request})\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    os.makedirs(\"static\", exist_ok=True)\n",
    "    os.makedirs(\"audio/user\", exist_ok=True)\n",
    "    os.makedirs(\"audio/ai\", exist_ok=True)\n",
    "    os.makedirs(\"embeddings_cache\", exist_ok=True)\n",
    "    os.makedirs(\"templates\", exist_ok=True)\n",
    "    with open(\"templates/index.html\", \"w\") as f:\n",
    "        f.write(\"\"\"<meta http-equiv=\"refresh\" content=\"0; url=/setup\" />\"\"\")\n",
    "    try:\n",
    "        torch.hub.load('snakers4/silero-vad', model='silero_vad', force_reload=False)\n",
    "    except: pass\n",
    "    asyncio.create_task(process_message_queue())\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    logger.info(\"Server shutting down...\")\n",
    "\n",
    "from flask import Flask, jsonify, request, send_file\n",
    "\n",
    "@app.get(\"/api/conversations\")\n",
    "async def get_conversations(request: Request):\n",
    "    conn = sqlite3.connect(\"companion.db\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT id, user_message, ai_message FROM conversations ORDER BY id DESC\")\n",
    "    data = [{\"id\": row[0], \"user_message\": row[1], \"ai_message\": row[2]} for row in cur.fetchall()]\n",
    "    conn.close()\n",
    "    return JSONResponse(content=data)\n",
    "\n",
    "@app.route(\"/api/conversations/<int:conv_id>\", methods=[\"PUT\"])\n",
    "def update_conversation(conv_id):\n",
    "    data = request.get_json()\n",
    "    conn = sqlite3.connect(\"companion.db\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"UPDATE conversations SET user_message=?, ai_message=? WHERE id=?\",\n",
    "                (data[\"user_message\"], data[\"ai_message\"], conv_id))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return \"\", 204\n",
    "\n",
    "@app.delete(\"/api/conversations\")\n",
    "async def delete_all_conversations():\n",
    "    try:\n",
    "        conn = sqlite3.connect(\"companion.db\")\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"DELETE FROM conversations\")\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return {\"status\": \"all deleted\"}\n",
    "    except Exception as e:\n",
    "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
    "\n",
    "@app.delete(\"/api/conversations/{conv_id}\")\n",
    "async def delete_conversation(conv_id: int):\n",
    "    try:\n",
    "        conn = sqlite3.connect(\"companion.db\")\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"DELETE FROM conversations WHERE id = ?\", (conv_id,))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return JSONResponse(content={\"status\": \"deleted\", \"id\": conv_id})\n",
    "    except Exception as e:\n",
    "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
    "\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "@app.get(\"/crud\", response_class=HTMLResponse)\n",
    "async def crud_ui(request: Request):\n",
    "    return templates.TemplateResponse(\"crud.html\", {\"request\": request})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    threading.Thread(target=lambda: asyncio.run(loop.run_forever()), daemon=True).start()\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627a053-1cb3-47ce-ad4b-e1af327c89ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSM-Streaming Environment",
   "language": "python",
   "name": "venvcsm.streaming"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
